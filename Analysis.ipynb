{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c2aef5",
   "metadata": {},
   "source": [
    "## Anna Kaniowska - Cellphones & Accessories dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505a6c1",
   "metadata": {},
   "source": [
    "The goal of this project is to extract as much information form the data set that can be obtained here - http://snap.stanford.edu/data/amazon/Cell_Phones_&_Accessories.txt.gz (source: http://snap.stanford.edu/data/web-Amazon-links.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports needed to perform the analysis\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415de27",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modified version of function available on the source page\n",
    "def parse(filename):\n",
    "    \"\"\"\n",
    "    Parses a gzipped text file and returns a list of dictionaries containing the parsed data.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the gzipped text file to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing the parsed data.\n",
    "    \"\"\"\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    entry = {}\n",
    "    data = []\n",
    "    for line in f:\n",
    "        l = line.decode('utf-8').strip()\n",
    "        colonPos = l.find(':')\n",
    "        if colonPos == -1:\n",
    "            data.append(entry)\n",
    "            entry = {}\n",
    "            continue\n",
    "        eName = l[:colonPos]\n",
    "        rest = l[colonPos+2:]\n",
    "        entry[eName] = rest\n",
    "    data.append(entry)\n",
    "    return data\n",
    "\n",
    "# Loading the dataset\n",
    "data = parse('Cell_Phones_&_Accessories.txt.gz')\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff9746",
   "metadata": {},
   "source": [
    "#### Getting to know the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447791f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the dataset is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780ac8a",
   "metadata": {},
   "source": [
    "Taking a first look at the data, it is visible that it shows the reviews that customers gave to the products. The products are mainly cellphones and their accesories. The dataset is big - almost 79 000 rows is a significant amount of data. 10 columns provide information about the rated product, the customer and their opinion on the product. When it comes to technical details - it is necessary to change 'unknown' values to NaN in order to prepare data to further analysis. Checking the dataset for duplicated rows and dropping existing ones is also necessary because this is something that cannot be seen at first glance. Another conclusion is that the types of the columns are not necessarily correct (e.g. product/price should be stored as float), it is also needed to be corrected.\n",
    "\n",
    "#### Checking for duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba20d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {df.duplicated().sum()} duplicated rows in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edc49b",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n",
    "# Analyzing the missing values occurences\n",
    "print(\"Missing values occurences:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Checking anonymous reviews (those where userId and profileName is missing)\n",
    "anon_reviews_perc = df['review/userId'].isna().sum()/df.shape[0] * 100\n",
    "print(f\"{anon_reviews_perc:.2f}% of the reviews are anonymously submitted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2f306",
   "metadata": {},
   "source": [
    "The first conclusion is one row that can be safely deleted in each column (it is very likely that it is the same row for each of the columns). \\\n",
    "The second conclusion refers to anonymous reviews. When missing values are less than 5% of given feature, they can be safely deleted without having impact on further analysis. For now the data will be divided into to sets - first with anonymous reviews and second with named reviews. It may be useful to further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac16955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns with 1 missing value\n",
    "cols = ['product/productId', 'product/title', 'review/helpfulness', 'review/score', 'review/time', \\\n",
    "        'review/summary', 'review/text']\n",
    "df = df.dropna(subset=cols)\n",
    "print(f\"Shape of the dataset after dropping NaNs: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131debf",
   "metadata": {},
   "source": [
    "As expected, only one row of the data was deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67d5e0",
   "metadata": {},
   "source": [
    "Before dividing dataset into two separate dataset it would be useful to correct the columns' types.\n",
    "\n",
    "#### Correcting columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product/price'] = pd.to_numeric(df['product/price'], errors='coerce')\n",
    "df['review/score'] = pd.to_numeric(df['review/score'], errors='coerce')\n",
    "df['review/time'] = pd.to_datetime(df['review/time'].astype(float), unit='s')\n",
    "\n",
    "def handle_helpfulness(x):\n",
    "    \"\"\"\n",
    "    Converts the string representation of helpfulness scores to a float value between 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "        helpfulness (str): The string representation of helpfulness scores, in the format \"x/y\",\n",
    "        where \"x\" is the number of users who found the review helpful and \"y\" is the total number of votes.\n",
    "\n",
    "    Returns:\n",
    "        float: The float value of the helpfulness score, calculated as \"x / y\". Returns 0 if \"y\" is 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nom, denom = x.split(\"/\")\n",
    "        return int(nom) / int(denom)\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return 0\n",
    "\n",
    "df['review/helpfulness'] = df['review/helpfulness'].apply(handle_helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b74626",
   "metadata": {},
   "source": [
    "The price and score columns are stroing numeric values, the review has time in seconds. When it comes to helpfulness it was transformed to the float value that represents it.\n",
    "\n",
    "Coming back to the missing values before the set division, the last thing about them is the price. As it is a significant amount of data in the dataset it would not make sense to drop it. Taking into consideration that assigning a price of a small accessory to a brand new cellphone would distort the dataset, the missing values in this column will not be replaced with mean, median or mode. The products are stored in more or less an order (similiar products next to one another) so an optimal way to impute the missing values would be kNN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53813654",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_missing_values = df[['product/price']]\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_features = imputer.fit_transform(features_with_missing_values)\n",
    "df['product/price'] = imputed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73381726",
   "metadata": {},
   "source": [
    "#### Division of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into anonymous and named\n",
    "df_anon = df[(df['review/userId'].isna()) & (df['review/profileName'].isna())]\n",
    "df_named = df[~(df['review/userId'].isna()) & ~(df['review/profileName'].isna())]\n",
    "\n",
    "# Deleting unnecessary columns from the anonymous reviews\n",
    "df_anon = df_anon.drop(columns=['review/userId', 'review/profileName'])\n",
    "\n",
    "# Checking if everything went as expected\n",
    "print(f\"Missing values occurences in anonymous reviews dataset:\\n{df_anon.isna().sum()}\")\n",
    "print(f\"Missing values occurences in named reviews dataset:\\n{df_named.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572299b7",
   "metadata": {},
   "source": [
    "The analysis will focus on the named reviews but there will always be an available point of reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c003d3",
   "metadata": {},
   "source": [
    "#### Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41570c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the columns with numercial and categorical variables\n",
    "numerical = ['product/price', 'review/helpfulness', 'review/score', 'review/time']\n",
    "categorical = ['product/productId', 'product/title', 'review/userId', 'review/profileName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b83a3",
   "metadata": {},
   "source": [
    "Summary and text would be difficult to visualize so those columns are not taken into consideration in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331c3ef",
   "metadata": {},
   "source": [
    "#### a. Named Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "fig, axs = plt.subplots(2, 2, figsize=[15, 15])\n",
    "fig.suptitle(\"Data distribution for Named Reviews\")\n",
    "\n",
    "max_counts = [df_named[col].value_counts().max() for col in numerical]\n",
    "for i, col in enumerate(df_named[numerical]):\n",
    "    sns.histplot(data=df_named, x=col, ax=axs[i//2, i%2], color='darkmagenta')\n",
    "    axs[i//2, i%2].set(title=col, xlabel='Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2c860",
   "metadata": {},
   "source": [
    "It is visible that most of the reviews are rather extremely helpful or extremely unhelpful, something in between is not seen very often. A similiar situation can be observed in the Score histogram but not to such extent (4.0 note is observed more often than 1.0). Majority of reviews has a score 5.0 which is a sign of good quality of the products. What is interesting is that most of the reviews were registered in 2007-2008 which is the time of Global Financial Crisis. The plot which refers to the price is not very transparent, so in order to better understand the data there was a boxplot created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff372f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,6])\n",
    "sns.boxplot(x=df_named['product/price'], ax=ax, color='darkmagenta')\n",
    "ax.set(xlabel='Product Price', title=\"Boxplot for Product Price (Named Reviews)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e62d9",
   "metadata": {},
   "source": [
    "It is important to remember that more than a half of the price values were imputed by kNN algorithm so the results might not be 100% reliable. That is also the reason why outliers are not removed. Majority of the product is in the cheaper section, while more expensive products are bought less often - they are treated as outliers in this dataset. Half of the dataset (between first and third quartile) is represented by (15,35) range (approximately)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4191",
   "metadata": {},
   "source": [
    "#### b. Anonymous Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "fig, axs = plt.subplots(2, 2, figsize=[15, 15])\n",
    "fig.suptitle(\"Data distribution for Anonymous Reviews\")\n",
    "\n",
    "max_counts = [df_anon[col].value_counts().max() for col in numerical]\n",
    "for i, col in enumerate(df_anon[numerical]):\n",
    "    sns.histplot(data=df_anon, x=col, ax=axs[i//2, i%2], color='darkmagenta')\n",
    "    axs[i//2, i%2].set(title=col, xlabel='Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd31834",
   "metadata": {},
   "source": [
    "The result are similar to those from Names Reviews dataset. The biggest difference is visible on the last plot - anonymous reviews were the most popular in 2004. It might prove that people in this time were not trusting the internet as much as they did in 2007-2008 and did not want to provide the personal data to any websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c7f87",
   "metadata": {},
   "source": [
    "#### Positive vs. Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3396fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_named['sentiment'] = df_named['sentiment'].map(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_named['review/text'] = df_named['review/text'].apply(preprocess_text)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_named['review/text'])\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_named['sentiment'], test_size=0.25, random_state=42)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the performance of the model on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_named['review/text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
