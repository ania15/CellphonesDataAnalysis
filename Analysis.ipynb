{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086e539b",
   "metadata": {},
   "source": [
    "## Anna Kaniowska - Cellphones & Accessories dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868fb45",
   "metadata": {},
   "source": [
    "The goal of this project is to extract as much information form the data set that can be obtained here - http://snap.stanford.edu/data/amazon/Cell_Phones_&_Accessories.txt.gz (source: http://snap.stanford.edu/data/web-Amazon-links.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23dd8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports needed to perform the analysis\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf8bc9",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modified version of function available on the source page\n",
    "def parse(filename):\n",
    "    \"\"\"\n",
    "    Parses a gzipped text file and returns a list of dictionaries containing the parsed data.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the gzipped text file to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing the parsed data.\n",
    "    \"\"\"\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    entry = {}\n",
    "    data = []\n",
    "    for line in f:\n",
    "        l = line.decode('utf-8').strip()\n",
    "        colonPos = l.find(':')\n",
    "        if colonPos == -1:\n",
    "            data.append(entry)\n",
    "            entry = {}\n",
    "            continue\n",
    "        eName = l[:colonPos]\n",
    "        rest = l[colonPos+2:]\n",
    "        entry[eName] = rest\n",
    "    data.append(entry)\n",
    "    return data\n",
    "\n",
    "# Loading the dataset\n",
    "data = parse('Cell_Phones_&_Accessories.txt.gz')\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6371b5",
   "metadata": {},
   "source": [
    "#### Getting to know the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda496cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ef2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f320344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the dataset is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc154b",
   "metadata": {},
   "source": [
    "Taking a first look at the data, it is visible that it shows the reviews that customers gave to the products. The products are mainly cellphones and their accesories. The dataset is big - almost 79 000 rows is a significant amount of data. 10 columns provide information about the rated product, the customer and their opinion on the product. When it comes to technical details - it is necessary to change 'unknown' values to NaN in order to prepare data to further analysis. Checking the dataset for duplicated rows and dropping existing ones is also necessary because this is something that cannot be seen at first glance. Another conclusion is that the types of the columns are not necessarily correct (e.g. product/price should be stored as float), it is also needed to be corrected.\n",
    "\n",
    "#### Checking for duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {df.duplicated().sum()} duplicated rows in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068437d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3912e4",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ebe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n",
    "# Analyzing the missing values occurences\n",
    "print(\"Missing values occurences:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Checking anonymous reviews (those where userId and profileName is missing)\n",
    "anon_reviews_perc = df['review/userId'].isna().sum()/df.shape[0] * 100\n",
    "print(f\"{anon_reviews_perc:.2f}% of the reviews are anonymously submitted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e4df1",
   "metadata": {},
   "source": [
    "The first conclusion is one row that can be safely deleted in each column (it is very likely that it is the same row for each of the columns). \\\n",
    "The second conclusion refers to anonymous reviews. When missing values are less than 5% of given feature, they can be safely deleted without having impact on further analysis. For now the data will be divided into to sets - first with anonymous reviews and second with named reviews. It may be useful to further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns with 1 missing value\n",
    "cols = ['product/productId', 'product/title', 'review/helpfulness', 'review/score', 'review/time', \\\n",
    "        'review/summary', 'review/text']\n",
    "df = df.dropna(subset=cols)\n",
    "print(f\"Shape of the dataset after dropping NaNs: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fd725",
   "metadata": {},
   "source": [
    "As expected, only one row of the data was deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75dc1f",
   "metadata": {},
   "source": [
    "Before dividing dataset into two separate dataset it would be useful to correct the columns' types.\n",
    "\n",
    "#### Correcting columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9245b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product/price'] = pd.to_numeric(df['product/price'], errors='coerce')\n",
    "df['review/score'] = pd.to_numeric(df['review/score'], errors='coerce')\n",
    "df['review/time'] = pd.to_datetime(df['review/time'].astype(float), unit='s')\n",
    "\n",
    "def handle_helpfulness(x):\n",
    "    \"\"\"\n",
    "    Converts the string representation of helpfulness scores to a float value between 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "        helpfulness (str): The string representation of helpfulness scores, in the format \"x/y\",\n",
    "        where \"x\" is the number of users who found the review helpful and \"y\" is the total number of votes.\n",
    "\n",
    "    Returns:\n",
    "        float: The float value of the helpfulness score, calculated as \"x / y\". Returns 0 if \"y\" is 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nom, denom = x.split(\"/\")\n",
    "        return int(nom) / int(denom)\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return 0\n",
    "\n",
    "df['review/helpfulness'] = df['review/helpfulness'].apply(handle_helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3608c9",
   "metadata": {},
   "source": [
    "The price and score columns are stroing numeric values, the review has time in seconds. When it comes to helpfulness it was transformed to the float value that represents it.\n",
    "\n",
    "#### Division of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92696285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into anonymous and named\n",
    "df_anon = df[(df['review/userId'].isna()) & (df['review/profileName'].isna())]\n",
    "df_named = df[~(df['review/userId'].isna()) & ~(df['review/profileName'].isna())]\n",
    "\n",
    "print(f\"Missing values occurences in anonymous reviews dataset:\\n{df_anon.isna().sum()}\")\n",
    "print(f\"Missing values occurences in named reviews dataset:\\n{df_named.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883dbdb8",
   "metadata": {},
   "source": [
    "The last thing about missing values is the price. As it is a significant amount of data in both datasets it would not make sense to drop it. After seeing more details of the data a desicion will be made whether to replace the missing values with mean, mode or median or to create a new category for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31069ccb",
   "metadata": {},
   "source": [
    "#### Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980869b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
