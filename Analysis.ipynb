{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c2aef5",
   "metadata": {},
   "source": [
    "## Anna Kaniowska - Cellphones & Accessories dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505a6c1",
   "metadata": {},
   "source": [
    "The goal of this project is to extract as much information form the data set that can be obtained here - http://snap.stanford.edu/data/amazon/Cell_Phones_&_Accessories.txt.gz (source: http://snap.stanford.edu/data/web-Amazon-links.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports needed to perform the analysis\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415de27",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modified version of function available on the source page\n",
    "def parse(filename):\n",
    "    \"\"\"\n",
    "    Parses a gzipped text file and returns a list of dictionaries containing the parsed data.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The path to the gzipped text file to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing the parsed data.\n",
    "    \"\"\"\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    entry = {}\n",
    "    data = []\n",
    "    for line in f:\n",
    "        l = line.decode('utf-8').strip()\n",
    "        colonPos = l.find(':')\n",
    "        if colonPos == -1:\n",
    "            data.append(entry)\n",
    "            entry = {}\n",
    "            continue\n",
    "        eName = l[:colonPos]\n",
    "        rest = l[colonPos+2:]\n",
    "        entry[eName] = rest\n",
    "    data.append(entry)\n",
    "    return data\n",
    "\n",
    "# Loading the dataset\n",
    "data = parse('Cell_Phones_&_Accessories.txt.gz')\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff9746",
   "metadata": {},
   "source": [
    "#### Getting to know the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c0f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447791f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the dataset is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780ac8a",
   "metadata": {},
   "source": [
    "Taking a first look at the data, it is visible that it shows the reviews that customers gave to the products. The products are mainly cellphones and their accesories. The dataset is big - almost 79 000 rows is a significant amount of data. 10 columns provide information about the rated product, the customer and their opinion on the product. When it comes to technical details - it is necessary to change 'unknown' values to NaN in order to prepare data to further analysis. Checking the dataset for duplicated rows and dropping existing ones is also necessary because this is something that cannot be seen at first glance. Another conclusion is that the types of the columns are not necessarily correct (e.g. product/price should be stored as float), it is also needed to be corrected.\n",
    "\n",
    "## Feature Engineering & Exploratory Data Analysis\n",
    "\n",
    "#### Checking for duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba20d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {df.duplicated().sum()} duplicated rows in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edc49b",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n",
    "# Analyzing the missing values occurences\n",
    "print(\"Missing values occurences:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Checking anonymous reviews (those where userId and profileName is missing)\n",
    "anon_reviews_perc = df['review/userId'].isna().sum()/df.shape[0] * 100\n",
    "print(f\"{anon_reviews_perc:.2f}% of the reviews are anonymously submitted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2f306",
   "metadata": {},
   "source": [
    "The first conclusion is one row that can be safely deleted in each column (it is very likely that it is the same row for each of the columns). \\\n",
    "The second conclusion refers to anonymous reviews. When missing values are less than 5% of given feature, they can be safely deleted without having impact on further analysis. For now the data will be divided into to sets - first with anonymous reviews and second with named reviews. It may be useful to further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac16955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns with 1 missing value\n",
    "cols = ['product/productId', 'product/title', 'review/helpfulness', 'review/score', 'review/time', \\\n",
    "        'review/summary', 'review/text']\n",
    "df = df.dropna(subset=cols)\n",
    "print(f\"Shape of the dataset after dropping NaNs: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131debf",
   "metadata": {},
   "source": [
    "As expected, only one row of the data was deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67d5e0",
   "metadata": {},
   "source": [
    "Before dividing dataset into two separate dataset it would be useful to correct the columns' types.\n",
    "\n",
    "#### Correcting columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product/price'] = pd.to_numeric(df['product/price'], errors='coerce')\n",
    "df['review/score'] = pd.to_numeric(df['review/score'], errors='coerce')\n",
    "df['review/time'] = pd.to_datetime(df['review/time'].astype(float), unit='s')\n",
    "\n",
    "def handle_helpfulness(x):\n",
    "    \"\"\"\n",
    "    Converts the string representation of helpfulness scores to a float value between 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "        helpfulness (str): The string representation of helpfulness scores, in the format \"x/y\",\n",
    "        where \"x\" is the number of users who found the review helpful and \"y\" is the total number of votes.\n",
    "\n",
    "    Returns:\n",
    "        float: The float value of the helpfulness score, calculated as \"x / y\". Returns 0 if \"y\" is 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nom, denom = x.split(\"/\")\n",
    "        return int(nom) / int(denom)\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return 0\n",
    "\n",
    "df['review/helpfulness'] = df['review/helpfulness'].apply(handle_helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b74626",
   "metadata": {},
   "source": [
    "The price and score columns are stroing numeric values, the review has time in seconds. When it comes to helpfulness it was transformed to the float value that represents it.\n",
    "\n",
    "Coming back to the missing values before the set division, the last thing about them is the price. As it is a significant amount of data in the dataset it would not make sense to drop it. Taking into consideration that assigning a price of a small accessory to a brand new cellphone would distort the dataset, the missing values in this column will not be replaced with mean, median or mode. The products are stored in more or less an order (similiar products next to one another) so an optimal way to impute the missing values would be kNN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53813654",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_missing_values = df[['product/price']]\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_features = imputer.fit_transform(features_with_missing_values)\n",
    "df['product/price'] = imputed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73381726",
   "metadata": {},
   "source": [
    "#### Division of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into anonymous and named\n",
    "df_anon = df[(df['review/userId'].isna()) & (df['review/profileName'].isna())]\n",
    "df_named = df[~(df['review/userId'].isna()) & ~(df['review/profileName'].isna())]\n",
    "\n",
    "# Deleting unnecessary columns from the anonymous reviews\n",
    "df_anon = df_anon.drop(columns=['review/userId', 'review/profileName'])\n",
    "\n",
    "# Checking if everything went as expected\n",
    "print(f\"Missing values occurences in anonymous reviews dataset:\\n{df_anon.isna().sum()}\")\n",
    "print(f\"Missing values occurences in named reviews dataset:\\n{df_named.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572299b7",
   "metadata": {},
   "source": [
    "The analysis will focus on the named reviews but there will always be an available point of reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c003d3",
   "metadata": {},
   "source": [
    "#### Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41570c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the columns with numercial and categorical variables\n",
    "numerical = ['product/price', 'review/helpfulness', 'review/score', 'review/time']\n",
    "categorical = ['product/productId', 'product/title', 'review/userId', 'review/profileName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b83a3",
   "metadata": {},
   "source": [
    "Summary and text would be difficult to visualize so those columns are not taken into consideration in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331c3ef",
   "metadata": {},
   "source": [
    "#### a. Named Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "fig, axs = plt.subplots(2, 2, figsize=[15, 15])\n",
    "fig.suptitle(\"Data distribution for Named Reviews\")\n",
    "\n",
    "max_counts = [df_named[col].value_counts().max() for col in numerical]\n",
    "for i, col in enumerate(df_named[numerical]):\n",
    "    sns.histplot(data=df_named, x=col, ax=axs[i//2, i%2], color='darkmagenta')\n",
    "    axs[i//2, i%2].set(title=col, xlabel='Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2c860",
   "metadata": {},
   "source": [
    "It is visible that most of the reviews are rather extremely helpful or extremely unhelpful, something in between is not seen very often. A similiar situation can be observed in the Score histogram but not to such extent (4.0 note is observed more often than 1.0). Majority of reviews has a score 5.0 which is a sign of good quality of the products. What is interesting is that most of the reviews were registered in 2007-2008 which is the time of Global Financial Crisis. The plot which refers to the price is not very transparent, so in order to better understand the data there was a boxplot created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff372f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8,6])\n",
    "sns.boxplot(x=df_named['product/price'], ax=ax, color='darkmagenta')\n",
    "ax.set(xlabel='Product Price', title=\"Boxplot for Product Price (Named Reviews)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e62d9",
   "metadata": {},
   "source": [
    "It is important to remember that more than a half of the price values were imputed by kNN algorithm so the results might not be 100% reliable. That is also the reason why outliers are not removed. Majority of the product is in the cheaper section, while more expensive products are bought less often - they are treated as outliers in this dataset. Half of the dataset (between first and third quartile) is represented by (15,35) range (approximately)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d4191",
   "metadata": {},
   "source": [
    "#### b. Anonymous Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "fig, axs = plt.subplots(2, 2, figsize=[15, 15])\n",
    "fig.suptitle(\"Data distribution for Anonymous Reviews\")\n",
    "\n",
    "max_counts = [df_anon[col].value_counts().max() for col in numerical]\n",
    "for i, col in enumerate(df_anon[numerical]):\n",
    "    sns.histplot(data=df_anon, x=col, ax=axs[i//2, i%2], color='darkmagenta')\n",
    "    axs[i//2, i%2].set(title=col, xlabel='Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd31834",
   "metadata": {},
   "source": [
    "The result are similar to those from Names Reviews dataset. The biggest difference is visible on the last plot - anonymous reviews were the most popular in 2004. It might prove that people in this time were not trusting the internet as much as they did in 2007-2008 and did not want to provide the personal data to any websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c7f87",
   "metadata": {},
   "source": [
    "## Machine Learning analysis\n",
    "\n",
    "#### Natural Language Processing - data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3396fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare necessary tools\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by tokenizing it into words, converting all words to lowercase, \n",
    "    removing any non-alphabetic characters and stop words, and stemming the remaining words. \n",
    "    The processed words are then joined into a single string and returned.\n",
    "\n",
    "    Args:\n",
    "    text (str): A string of text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed text, consisting of stemmed words in lower case, with non-alphabetic characters and stop words removed and all words joined into a single string.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [w for w in words if w.isalpha() and w not in stop_words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df_named['text'] = df_named['review/summary'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b9fd3",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6fc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "df_named['sentiment'] = df_named['review/score'].apply(lambda score: 1 if score > 3 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_named['text'], df_named['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data into a numerical representation using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258984bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10% of the rows randomly\n",
    "sample_df = df_named[:20000]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67efa0",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data into a numerical representation using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(batches[0]['text'])\n",
    "\n",
    "# Determine the optimal number of clusters using elbow method\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters and fit the K-means algorithm\n",
    "k =8\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Evaluate the clustering performance using silhouette score\n",
    "labels = kmeans.labels_\n",
    "score = silhouette_score(X, labels)\n",
    "print(f'Silhouette score: {score}')\n",
    "\n",
    "# Interpret the clusters to determine the common themes or topics in the comments\n",
    "for i in range(k):\n",
    "    cluster_idx = labels == i\n",
    "    cluster_text = batches[0].loc[cluster_idx, 'review/summary']\n",
    "    print(f'Cluster {i}:')\n",
    "    print(cluster_text.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f96ea",
   "metadata": {},
   "source": [
    "#### Recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39231d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TF-IDF matrix of the product titles\n",
    "vectorizer = TfidfVectorizer()\n",
    "product_titles = vectorizer.fit_transform(sample_df['product/title'])\n",
    "\n",
    "# compute the cosine similarity matrix of the product titles\n",
    "cosine_sim_matrix = cosine_similarity(product_titles)\n",
    "\n",
    "# define a function to get the top n similar products\n",
    "def get_similar_products(product_id, n=5):\n",
    "    unique_products = sample_df.groupby('product/productId').first().reset_index()\n",
    "\n",
    "    # get the row index of the product in the cosine similarity matrix\n",
    "    idx = unique_products.index[unique_products['product/productId'] == product_id].tolist()[0]\n",
    "    \n",
    "    # get the cosine similarity scores between the product and all other products\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "    \n",
    "    # sort the scores in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # get the indices of the top n similar products (excluding itself)\n",
    "    top_indices = [i for i, _ in sim_scores[1:n+1]]\n",
    "    \n",
    "    # return the product IDs and titles of the top n similar products\n",
    "    return unique_products.loc[top_indices, ['product/productId', 'product/title']]\n",
    "\n",
    "# example usage: get the top 5 similar products to product with ID 'B000JVER7W'\n",
    "get_similar_products('B000JIK92C')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
